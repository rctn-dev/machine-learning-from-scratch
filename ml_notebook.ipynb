{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING FROM SCRATCH    \n",
    "* Machine learning is the field of study that gives computers the ability to <span style=\"color:orange\">learn without being explicitly programmmed</span>.  \n",
    "* Two main types of machine learning are <span style=\"color:orange\">supervised learning</span> and <span style=\"color:orange\">unsupervised learning</span>.  \n",
    "* Of these two, the supervised learning is the type of machine learning that is used most in many real-world applications and has seen the most rapid advancements and innovation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Supervised Learning  \n",
    "* The 99% of the economic value created by machine learning today is through one type of machine learning, which is called supervised learning.\n",
    "* Supervised learning maps input (x) to output (y), where the learning algorithm learns from the <span style=\"color:orange\">right</span> answers.\n",
    "* Key characteristic of supervised learning is that you give your learning algorithm examples to learn from, called <span style=\"color:orange\">training set</span>.\n",
    "* The two major types of supervised learning are <span style=\"color:orange\">regression</span> and <span style=\"color:orange\">classification</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Regression\n",
    "* In a regression application, the learning algorithm has to predict numbers from within <span style=\"color:orange\">infinitely-many options</span>. In other words, it is used to <span style=\"color:orange\">predict continuous values</span>. To clarify, assume you have a prediction function which might pick any real number within the real number range of (0,1) as an output result such as 0.19, 0.73456, or 0.2717876756.\n",
    "    * The House price prediction problem is a typical example of these kinds of problems.\n",
    "* The ultimate goal of the regression algorithm is to <span style=\"color:orange\">plot a best-fit line or a curve</span> between the data.\n",
    "* There are right answers, i.e., labels, within the given dataset since it is still supervised learning.\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"50%\" alt=\"house price prediction\" src=\"assets/images/house_pricing_prediction.svg\"/> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Linear Regression\n",
    "* Linear regression model means fitting a straight line to your data, which is probably the most widely used learning algorithm in the world today.\n",
    "*  The goal is to train the model $f$, for which you feed the training dataset, both the input features ($x$) and the output targets ($y$), to your learning algorithm. Then your supervised learning algorithm will produce some function (model) of $f$. The job of $f$ is to take a new input of $x$ and output an estimate (prediction) of $\\hat{y}$. \n",
    "* E.g., the linear regression model with one variable (single feature of $x$; i.e., $x$ should be a vector for representing multiple features of $x_{1}$, $x_{2}$, $x_{3}$, ... )\n",
    "\n",
    "    $\\hat{y}=f_{w,b}(x)=f(w,b,x)=wx+b$ \n",
    "    \n",
    "* Some conventional terms utilized while discussing a model.  \n",
    "\n",
    "\n",
    "| Term      | Description| Example\n",
    "| :-----------: |:-----------|:-------------|\n",
    "|$x$| input features|size of a house in the training dataset\n",
    "|$y$| output targets (right answers)| actual price of a house in the training dataset\n",
    "|$f$| model| house price prediction model for various sizes\n",
    "|$\\hat{y}$| predicted (estimated) output| the predicted price of a house \n",
    "|$(x^{i},y^{i})$| $i^{th}$ training sample| size and actual price of $i^{th}$ house in the training-set\n",
    "|$m$| number of training samples in the dataset|\n",
    "|$w$| model parameter: weight|\n",
    "|$b$| model  parameter: bias|\n",
    "|$J(w,b)$| total cost coming from all training dataset samples.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Cost Function\n",
    "\n",
    "* The model can not exactly match for all actual output targets (right answers) simultaneously, there should be an ensemble error (cost). E.g. for the $i^{th}$ sample of the training dataset, $\\hat{y}^{i} \\neq y^{i}$, still the expected and the actual output might be equal for some samples. However, the total error (cost) ($J$) is the important concept here. The cost function for linear model with single feature case is as follows, which is normalized with ($2m$) for preventing the cost function from diverging as the size of the dataset ($m$) increases.\n",
    "\n",
    "$J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (\\hat{y}^i-y^i)^2$, for a generic case. \n",
    "    \n",
    "$J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (w{x}^i+b-y^i)^2$, for the specific case where a linear model with single feature is utilized.\n",
    "\n",
    "* The goal of the learning algorithm (e.g. the gradient descent algorithm) is to minimize the cost function. \n",
    "\n",
    "* (Put some explanatary figure.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Training with Gradient Descent\n",
    "* For a single feature linear model, $J$ is just a scalar value dependent on weight ($w$) and bias ($b$) scalar unknowns. These scalars are unknown for now since the learning algorithm is not trained yet with training data ($x^{i}, y^{i}$), which are known to us.\n",
    "* While ($w$) and ($b$) can be determined explicitely for simple cost ($J$) functions by simply examining the derivative,i.e., making it zero, it might not be possible for more complex cost functions.\n",
    "* Automate the process of optimizing $w$ and $b$ using gradient descent.\n",
    "* The gradient descent algorithm offers a numerical solution for automatically finding the minimum of complex cost functions as follows.\n",
    "    1. Choose a random starting point for weight and bias values, i.e., ($w^k, b^k$).\n",
    "    2. Consider that the solution set of ($w,b$) forming a vector space, in other words, each ($w,b$) item is a vector, and find the vector direction of maximum increase in cost function $J$, which describes a well-known concept in Calculus, which is the directional derivative of a scalar function.\n",
    "    3. Since we are looking for maximum descent instead of an ascent, put a minus sign in front of the $\\nabla$ operator and estimate the next values of  ($w^{k+1}, b^{k+1}$) with some step values $\\alpha$, which is called the learning rate until a convergence condition is satisfied.\n",
    "\n",
    "$\\theta^{k+1}=\\theta^{k}-\\alpha\\nabla{J}$\n",
    "* For $\\theta^{k}=(w^k,b^k)$, th gradient is as follows,\n",
    "\n",
    "$\\nabla{J}=\\frac{\\partial{J}}{\\partial{w}}\\hat{w}+\\frac{\\partial{J}}{\\partial{b}}\\hat{b}$\n",
    "\n",
    "\n",
    "* The expressions with $\\hat{w}$ and $\\hat{b}$ correspond to unit vector directions. \n",
    "Remembering the fact that matrices can also be considered as shorthands of vector repreentations, the following expression is also employed.\n",
    "\n",
    "$\\left[\\begin{array}{c}\n",
    "\\frac{\\partial{J}}{\\partial{w}} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial{J}}{\\partial{b}} \n",
    "\\end{array}\\right]$ \n",
    "\n",
    "* Applying partial derivatives for linear regression case.\n",
    "\n",
    "$J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (w{x}^i+b-y^i)^2$\n",
    "\n",
    "$\\frac{\\partial{J(w,b)}}{\\partial{w}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (w{x}^i+b-y^i)x^i$\n",
    "\n",
    "$\\frac{\\partial{J(w,b)}}{\\partial{b}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (w{x}^i+b-y^i)$\n",
    "\n",
    "* The iterative calculation is performed as follows, both of which should be updated simultaneously for each iteration.\n",
    "\n",
    "$w^{k+1}=w^{k}-\\alpha\\frac{\\partial{J}}{\\partial{w}}|_{(w^k,b^k)}$\n",
    "\n",
    "$b^{k+1}=b^{k}-\\alpha\\frac{\\partial{J}}{\\partial{b}}|_{(w^k,b^k)}$\n",
    "* A graphical representation of how gradient descent advances with iterations of $(w,b)$ for finding the local minima.\n",
    "<p>\n",
    "    <img width=\"40%\" alt=\"cost_3d\" src=\"assets/images/cost_3d.svg\"/> \n",
    "    <img width=\"40%\" alt=\"cost_contour\" src=\"assets/images/cost_contour.svg\"/> \n",
    "</p>\n",
    "\n",
    "* The choice of the learning rate, $\\alpha$ will have a huge impact on the efficiency of the implementation of gradient descent.\n",
    "    * If the learning rate is too small, then gradient descents will work, but it will be slow.\n",
    "    * If the learning rate is too large, the intersect may overshoot and may never reach the minimum, may fail to converge and may even diverge. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Vectorization - Multiple Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Classification \n",
    "* While the regression type learning tries to predict an output among infinitely-many options, the classification type tries to predict the output, i.e. classes or categories, within only a small number, i.e., <span style=\"color:orange\">finitely-many options</span>. \n",
    "* The ultimate goal of the classification algorithm is to <span style=\"color:orange\">find a boundary line or a curve</span> that seperates actual outputs.\n",
    "* Categories (classes) can be <span style=\"color:orange\">numeric</span> like 0, 1 or 0, 1, 2. Neverthelesss, they don't have to be numbers, they can be <span style=\"color:orange\">non-numeric</span>. To illustrate, the classification can predict whether a picture is that of a cat or a dog or it can predict if a tumor is benign or malignant.\n",
    "* There are right answers,i.e., labels, within the given dataset since it is still supervised learning. \n",
    "* Some examples of classification can be as follows;  \n",
    "    * Given email labeled as spam/not spam, design a spam filter.\n",
    "    * Given a dataset of patients diagnosed as either having diabetes or not, learn to classify new patients as having diabetes or not.\n",
    "    * Given pictures of dogs, cats, and wolves, learn to classify if a new picture is that of a dog, cat, or wolf.\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"50%\" alt=\"tumor prediction\" src=\"assets/images/tumor_prediction.svg\" /> \n",
    "</p>\n",
    "\n",
    "<!-- ![Picture](assets/images/house_pricing_prediction.svg){ width=\"800\" height=\"600\" style=\"display: block; margin: 0 auto\" } -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Unsupervised Learning\n",
    "* While the data comes with both inputs (x) and output labels (y) in supervised learning, the data comes <span style=\"color:orange\"> only with inputs (x)</span> in unsupervised learning  and the learning algorithm has to find some structure, some pattern or <span style=\"color:orange\">something interesting in the data.</span>\n",
    "* After supervised learning, the most widely used form of machine learning is unsupervised learning.\n",
    "* Assume that a dataset is given with patient's tumor size and the patient's age but not whether the tumor is benign or malignant.  Our job is to find some structure or some pattern or just find something interesting in the data. This is unsupervised learning, we call it unsupervised because we're not trying to supervise the algorithm.\n",
    "\n",
    "<p>\n",
    "    <img width=\"48%\" alt=\"peaks_3d\" src=\"assets/images/unsup_vs_sup_learning1.svg\"/> \n",
    "    <img width=\"48%\" alt=\"peaks_contour\" src=\"assets/images/unsup_vs_sup_learning2.svg\"/> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Clustering\n",
    "* The clustering algorithm is a type of unsupervised learning algorithm, which takes data without labels and tries to automatically group them into clusters.\n",
    "* A clustering algorithm groups similar data points together\n",
    "    * Given a database of customer data, automatically discover market segments and group customers into different market segments.\n",
    "    * Given a set of news articles found on the web, group them into sets of articles about the same stories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
