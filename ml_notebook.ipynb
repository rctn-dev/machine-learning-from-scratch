{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING FROM SCRATCH    \n",
    "* Machine learning is the field of study that gives computers the ability to <span style=\"color:orange\">learn without being explicitly programmmed</span>.  \n",
    "* Two main types of machine learning are <span style=\"color:orange\">supervised learning</span> and <span style=\"color:orange\">unsupervised learning</span>.  \n",
    "* Of these two, the supervised learning is the type of machine learning that is used most in many real-world applications and has seen the most rapid advancements and innovation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised Learning  \n",
    "* The 99% of the economic value created by machine learning today is through one type of machine learning, which is called supervised learning.\n",
    "* Supervised learning maps input (x) to output (y), where the learning algorithm learns from the <span style=\"color:orange\">right</span> answers.\n",
    "* Key characteristic of supervised learning is that you give your learning algorithm examples to learn from, called <span style=\"color:orange\">training set</span>.\n",
    "* The two major types of supervised learning are <span style=\"color:orange\">regression</span> and <span style=\"color:orange\">classification</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Regression\n",
    "* In a regression application, the learning algorithm has to predict numbers from within <span style=\"color:orange\">infinitely-many options</span>. In other words, it is used to <span style=\"color:orange\">predict continuous values</span>. To clarify, assume you have a prediction function which might pick any real number within the real number range of (0,1) as an output result such as 0.19, 0.73456, or 0.2717876756.\n",
    "    * The House price prediction based on size and many other attributes is a typical example of these kinds of problems.\n",
    "* The ultimate goal of the regression algorithm is to plot a  <span style=\"color:orange\">best-fit-line</span> or a <span style=\"color:orange\">best-fit-curve</span> between the data.\n",
    "* There are right answers, i.e., labels (values), within the given dataset since it is still supervised learning.\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"50%\" alt=\"house price prediction\" src=\"assets/images/house_pricing_prediction.svg\"/> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Linear Regression\n",
    "* Linear regression model means fitting a straight line to your data, which is probably the most widely used learning algorithm in the world today.\n",
    "*  The goal is to train the model $f$, for which you feed the training dataset, both the input features ($x$) and the output targets ($y$), to your learning algorithm. Then your supervised learning algorithm will produce some function (model) of $f$. The job of $f$ is to take a new input of $x$ and output an estimate (prediction) of $\\hat{y}$. \n",
    "* E.g., the linear regression model with one variable (single feature of $x$; i.e., $x$ should be a vector for representing multiple features of $x_{1}$, $x_{2}$, $x_{3}$, ... )\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\hat{y}=f_{w,b}(x)=f(w,b,x)=wx+b\n",
    "\\end{equation}\n",
    "    \n",
    "* Some conventional terms utilized while discussing a model.  \n",
    "\n",
    "\n",
    "| Term      | Description| Example\n",
    "| :-----------: |:-----------|:-------------|\n",
    "|$x$| input features|size of a house in the training dataset\n",
    "|$y$| output targets (right answers)| actual price of a house in the training dataset\n",
    "|$f$| model| house price prediction model for various sizes\n",
    "|$\\hat{y}$| predicted (estimated) output| the predicted price of a house \n",
    "|$(x_{i},y_{i})$| $i^{th}$ training sample| size and actual price of $i^{th}$ house in the training-set\n",
    "|$m$| number of training samples in the dataset|\n",
    "|$w$| model parameter: weight|\n",
    "|$b$| model  parameter: bias|\n",
    "|$J(w,b)$| total cost coming from all training dataset samples for each $(w,b)$ pair\n",
    "|$i$ subscript|$1<i<m$ corresponds to different items of training dataset \n",
    "|$j$ subscript|$1<j<n$ corresponds to different fatures of the model\n",
    "|$k$ subscript|$1<k< ? $ corresponds to iteration as gradient descent algo works\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Cost Function for Linear Regression\n",
    "\n",
    "* The model can not exactly match for all actual output targets (right answers) simultaneously, there should be an ensemble error (cost). E.g. for the $i^{th}$ sample of the training dataset, $\\hat{y}^{i} \\neq y^{i}$, still the expected and the actual output might be equal for some samples. However, the total error (cost) ($J$) is the important concept here. The cost function for linear model with single feature case is as follows, which is normalized with ($2m$) for preventing the cost function from diverging as the size of the dataset ($m$) increases.\n",
    "* For a generic case,\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (\\hat{y}_i-y_i)^2\n",
    "\\end{equation}\n",
    "* For the specific case where a linear model with single feature is utilized.\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (w{x}_i+b-y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "* The goal of the learning algorithm (e.g. the gradient descent algorithm) is to minimize the cost function. \n",
    "* A graphical representation of how gradient descent advances with iterations of $(w,b)$ for finding the local minima of the cost function $J$.\n",
    "* Different initial points for the gradient descent algorithm might result in different local minima.\n",
    "<p>\n",
    "    <img width=\"40%\" alt=\"cost_3d\" src=\"assets/images/cost_3d.png\"/> \n",
    "    <img width=\"40%\" alt=\"cost_contour\" src=\"assets/images/cost_contour.svg\"/> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Polynomial Regression\n",
    "* Take the ideas of multiple linear regression and feature engineering to come up with a new algorithm called polynomial regression, which will let you fit curves, non-linear functions, to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Gradient Descent Algorithm\n",
    "* For a single feature linear model, $J$ is just a scalar value dependent on weight ($w$) and bias ($b$) scalar unknowns. These scalars are unknown for now since the learning algorithm is not trained yet with training data ($x^{i}, y^{i}$), which are known to us.\n",
    "* While ($w$) and ($b$) can be determined explicitely for simple cost ($J$) functions by simply examining the derivative,i.e., making it zero, it might not be possible for more complex cost functions.\n",
    "* Automate the process of optimizing $w$ and $b$ using gradient descent.\n",
    "* The gradient descent algorithm offers a numerical solution for automatically finding the minimum of complex cost functions as follows.\n",
    "    1. Choose a random starting point for weight and bias values, i.e., ($w_k, b_k$).\n",
    "    2. Consider that the solution set of ($w,b$) forming a vector space, in other words, each ($w,b$) item is a vector, and find the vector direction of maximum increase in cost function $J$, which describes a well-known concept in Calculus, which is the directional derivative of a scalar function.\n",
    "    3. Since we are looking for maximum descent instead of an ascent, put a minus sign in front of the $\\nabla$ operator and estimate the next values of  ($w^{k+1}, b^{k+1}$) with some step values $\\alpha$, which is called the learning rate until a convergence condition is satisfied.\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\theta_{k+1}=\\theta_{k}-\\alpha\\nabla{J}\n",
    "\\end{equation}\n",
    "* For $\\theta_{k}=(w_k,b_k)$, the gradient and iterative calculations are performed as follows, both of which should be updated simultaneously for each iteration.\n",
    "<!-- \n",
    "$\\nabla{J}=\\frac{\\partial{J}}{\\partial{w}}\\hat{w}+\\frac{\\partial{J}}{\\partial{b}}\\hat{b}$ -->\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    w_{k+1}=w_{k}-\\alpha\\frac{\\partial{J}}{\\partial{w}}|_{(w_k,b_k)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    b_{k+1}=b_{k}-\\alpha\\frac{\\partial{J}}{\\partial{b}}|_{(w_k,b_k)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "* The expressions with $\\hat{w}$ and $\\hat{b}$ correspond to unit vector directions. \n",
    "Remembering the fact that matrices can also be considered as shorthands of vector repreentations, the following expression is also employed.\n",
    "\n",
    "<!-- $\\left[\\begin{array}{c}\n",
    "\\frac{\\partial{J}}{\\partial{w}} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial{J}}{\\partial{b}} \n",
    "\\end{array}\\right]$  -->\n",
    "\n",
    "## 3.1 Single Feature Linear Regression\n",
    "\n",
    "* Applying partial derivatives for linear regression case.\n",
    "<!-- \n",
    "$J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (w{x}^i+b-y^i)^2$ -->\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\frac{\\partial{J(w,b)}}{\\partial{w}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (w{x}_i+b-y_i)x_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\frac{\\partial{J(w,b)}}{\\partial{b}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (wx_i+b-y_i)\n",
    "\\end{equation}\n",
    "\n",
    "* The complete form of the gradient for linear regression case, which will NOT be used in the coding part, we will decompose it to simpler code segments for **single responsibility** purposes. \n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    w_{k+1}=w_{k}-\\frac{\\alpha}{m}\\sum\\limits_{i=1}^m (wx_i+b-y_i)x_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    b_{k+1}=b_{k}-\\frac{\\alpha}{m}\\sum\\limits_{i=1}^m (wx_i+b-y_i)\n",
    "\\end{equation}\n",
    "\n",
    "*  The implementation of gradient algorithm can be decomposed into smaller code segments such as *compute_cost* for the computation of $J$ and *compute_partial_gradients* for the computation of partial derivatives at each iteration and finally for obtaining the actual result with *compute_gradient_descent*. The algorithm should advance as follows:\n",
    "    * Start with a random initial value for the weight and bias values as $(w_{k}, b_{k})$.\n",
    "    * Calculate the cost at $k$, $J_{k}$ as the sum of error $(\\hat{y}_k-y)^{2}$.\n",
    "    * Calculate the next values of $(w_{k+1}, b_{k+1})$.\n",
    "    * Calculate the new cost at  $J_{k+1}$.\n",
    "    * Test the convergence, e.g. if  $J_{k+1}>J_{k+1}$ then stop, else update the $(w_{k}, b_{k})$ with $(w_{k+1}, b_{k+1})$ and continue the loop.  \n",
    "\n",
    "* The implementation code for single feature linear regression is as follows, where $w$ is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\CODE\\CODES\\7_ML_PROJECTS\\machine-learning-from-scratch\\ml_notebook.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/CODE/CODES/7_ML_PROJECTS/machine-learning-from-scratch/ml_notebook.ipynb#W6sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m b_init\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/CODE/CODES/7_ML_PROJECTS/machine-learning-from-scratch/ml_notebook.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m alpha\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/CODE/CODES/7_ML_PROJECTS/machine-learning-from-scratch/ml_notebook.ipynb#W6sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m x_train,y_train\u001b[39m=\u001b[39mload_data()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/CODE/CODES/7_ML_PROJECTS/machine-learning-from-scratch/ml_notebook.ipynb#W6sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m w_final,b_final,cost_history,wb_history\u001b[39m=\u001b[39mcompute_gradient_descent(x_train,y_train, w_init,b_init,alpha,compute_cost,compute_partial_gradients)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Gradient descent for linear regression with single-feature, where w is a scalar.\n",
    "import numpy as np\n",
    "def compute_cost(x_train,y_train,w,b):\n",
    "    ''' \n",
    "    Args:\n",
    "        x_train (np.array (m,)): training dataset, input features\n",
    "        y_train (np.array (m,)): training dataset, target values\n",
    "        w,b (scalar):model parameters\n",
    "    Return:\n",
    "        norm_cost (scalar): normalized total cost value.\n",
    "    '''\n",
    "    _cost=0\n",
    "    m=x_train.shape[0] # ndarray.shape outputs a tuple, e.g., (m,n) for a 2D matrix.\n",
    "    for i in range(m):\n",
    "        y_predict=w*x_train[i]+b\n",
    "        _cost+=(y_predict-y_train[i])**2\n",
    "    cost=_cost/2/m\n",
    "    return cost\n",
    "\n",
    "def compute_partial_gradients(x_train, y_train,w,b):\n",
    "    ''' \n",
    "    Args:\n",
    "        x_train (np.array (m,)): training dataset, input features\n",
    "        y_train (np.array (m,)): training dataset, target values\n",
    "        w,b (scalar):model parameters\n",
    "    Return:\n",
    "        dj_dw(scalar): partial gradient of the cost w.r.t the model parameter of w.\n",
    "        dj_db(scalar): partial gradient of the cost w.r.t the model parameter of b.\n",
    "    '''\n",
    "    _dj_dw=0\n",
    "    _dj_db=0\n",
    "    m=x_train.shape[0]\n",
    "    for i in range(m):\n",
    "        _dj_dw+=(w*x_train[i]+b-y_train[i])*x_train[i]\n",
    "        _dj_db+=(w*x_train[i]+b-y_train[i])\n",
    "    dj_dw,dj_db=_dj_dw/m,_dj_db/m\n",
    "    return (dj_dw,dj_db)\n",
    "\n",
    "def compute_gradient_descent(x_train,y_train, w_init,b_init,alpha,compute_cost,compute_partial_gradients):\n",
    "    ''' \n",
    "    Args:\n",
    "        x_train (np.array (m,)): training dataset, input features\n",
    "        y_train (np.array (m,)): training dataset, target values\n",
    "    Return:\n",
    "        w,b (scalar tuple): model parameters to be optimized.\n",
    "        cost_history (np.array): array of calculated costs while minimizing the cost function.\n",
    "        wb_history (np.array of (w,b) tuples): array of caculated (w,b) pairs while searching for optimum values.\n",
    "    '''\n",
    "    w_cur=w_init\n",
    "    b_cur=b_init\n",
    "    cost_next=1\n",
    "    cost_cur=0\n",
    "    cost_history=np.array([])\n",
    "    wb_history=np.array([])\n",
    "    while(cost_next<cost_cur):\n",
    "        cost_cur=compute_cost(x_train,y_train,w_cur,b_cur)\n",
    "        np.append(cost_history,cost_cur)\n",
    "        np.append(wb_history,(w_cur,b_cur))\n",
    "        (dj_dw,dj_db)=compute_partial_gradients(x_train, y_train,w_cur,b_cur)\n",
    "        w_next=w_cur-alpha*dj_dw\n",
    "        b_next=b_cur-alpha*dj_db\n",
    "        cost_next=compute_cost(x_train,y_train,w_next,b_next)\n",
    "        w_cur=w_next\n",
    "        b_cur=b_next\n",
    "    return (w_cur,b_cur), cost_history, wb_history\n",
    "\n",
    "w_init=0\n",
    "b_init=0\n",
    "alpha=1e-2\n",
    "x_train,y_train=load_data()\n",
    "w_final,b_final,cost_history,wb_history=compute_gradient_descent(x_train,y_train, w_init,b_init,alpha,compute_cost,compute_partial_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Multiple Feature Linear Regression - Vectorization\n",
    "* The multiple linear regression is probably the single most widely used learning algorithm in the world today.\n",
    "* For a typical multiple feature linear regression case, the estimated value is expressed as follows, where $x_{1}, x_{2}, x_{3}, ...,  x_{n}$ correspond for different features.\n",
    "\n",
    "| single-feature| multiple-feature|\n",
    "| :-----------: |:-----------|\n",
    "| $\\hat{y}=wx+b$| $\\hat{y}=w_{1} x_{1}+w_{2} x_{2}+w_{3} x_{3}+...+x_{n}+b$ \n",
    "\n",
    "* It is better to implement matrix formulation for a better picturing of the estimated output of multiple feature case and rationalize the vectorization.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\hat{y}=w_{1} x_{1}+w_{2} x_{2}+w_{3} x_{3}+...+x_{n}+b = \\bar{w}.\\bar{x}+b\n",
    "\\end{equation}\n",
    "\n",
    "*  where the bar sign corresponds for the vector version of the weights and features.\n",
    "* Vectorization has benefits when you're implementing a learning algorithm, such as:\n",
    "    * The code will be <span style=\"color:orange\">shorter</span>.\n",
    "    * The code will run <span style=\"color:orange\">faster</span>, much more efficiently.\n",
    "* Vectorized code will allow you to also take advantage of modern numerical linear algebra libraries, as well as maybe even GPU hardware, which is originally designed to speed up computer graphics in your computer, but it turns out that it can help you execute your code much more quickly when you write vectorized code.\n",
    "* Numpy is a numerical linear algebra library in Python, which is by far the most widely used numerical linear algebra library in Python and in machine learning.\n",
    "* NumPy dot function is a vectorized implementation of the dot product operation between two vectors and especially when n is large, which will run much faster.\n",
    "* The NumPy dot function is able to use parallel hardware in your computer and this is true whether you're running this on a <span style=\"color:orange\">normal-CPU </span> or on a <span style=\"color:orange\">GPU</span>.\n",
    "* Vectorized implementations of learning algorithms has been a key step to getting learning algorithms to run efficiently, and therefore <span style=\"color:orange\">scale-up</span> well to large datasets that many modern machine learning algorithms now have to operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Improving Gradient Descent\n",
    " \n",
    "\n",
    "## 4.1 Feature Scaling\n",
    "* Feature scaling will enable gradient descent to run much faster.\n",
    "* When you have different features that take on very different ranges of values, it can cause gradient descent to run slowly in order to prevent bouncing-back and forth but re-scaling the different features so they all take on comparable range of values can speed up gradient descent significantly. \n",
    "*  mean normalization and z-score normalization are two of various normalizationntechniques to re-scale the ranges.\n",
    "## 4.2 Learning Rate\n",
    "* Learning algorithm will run much better with an appropriate choice of learning rate. \n",
    "    * If it's too small, it will run very slowlY \n",
    "    * If it is too large, it may not even converge.\n",
    "* If you plot the cost for a number of iterations and notice that the costs sometimes goes up and sometimes goes down, you should take that as a clear sign that gradient descent is not working properly.\n",
    "    * This could mean that there's a bug in the code. \n",
    "    * Or sometimes it could mean that your learning rate is too large.\n",
    "* Note that setting $\\alpha$ to be really small is meant here as a debugging step and a very small value of $\\alpha$ is not going to be the most efficient choice for actually training your learning algorithm.\n",
    "## 4.3 Feature Engineering\n",
    "* The choice of features can have a huge impact on your learning algorithm's performance. In fact, for many practical applications, choosing or entering the right features is a critical step to making the algorithm work well.\n",
    "* Creating a new feature is an example of what's called feature engineering, in which you might use your knowledge or intuition about the problem to design new features usually by transforming or combining the original features of the problem in order to make it easier for the learning algorithm to make accurate predictions. \n",
    "* It turns out that this one flavor of feature engineering, that allow you to fit not just straight lines, but curves, non-linear functions to your data\n",
    "## 4.4 Convergence Condition\n",
    "* Number of iterations that gradient descent takes a conversion can vary a lot between different applications. In one application, it may converge after just 30 iterations. For a different application, it could take 1,000 or 100,000 iterations.\n",
    "* Let's let epsilon be a variable representing a small number, If the cost J decreases by less  than this number epsilon on one iteration, then you're likely on this flattened part of the curve that you see on the left and you can declare convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Classification \n",
    "* While the regression type learning tries to predict an output among infinitely-many options, the classification type tries to predict the output, i.e. classes or categories, within only a small number, i.e., <span style=\"color:orange\">finitely-many options</span>. \n",
    "* The ultimate goal of the classification algorithm is to find a <span style=\"color:orange\"> boundary-line </span> or a <span style=\"color:orange\">boundary-curve</span> that seperates actual outputs.\n",
    "* <span style=\"color:orange\">Categories (classes)</span> can be <span style=\"color:orange\">numeric</span> like 0, 1 or 0, 1, 2. Neverthelesss, they don't have to be numbers, they can be <span style=\"color:orange\">non-numeric</span>. To illustrate, the classification can predict whether a picture is that of a cat or a dog or it can predict if a tumor is benign or malignant.\n",
    "* There are right answers,i.e., labels, within the given dataset since it is still supervised learning. \n",
    "* Some examples of classification can be as follows;  \n",
    "    * Given email labeled as spam/not spam, design a spam filter.\n",
    "    * Given a dataset of patients diagnosed as either having diabetes or not, learn to classify new patients as having diabetes or not.\n",
    "    * Given pictures of dogs, cats, and wolves, learn to classify if a new picture is that of a dog, cat, or wolf.\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"50%\" alt=\"tumor prediction\" src=\"assets/images/tumor_prediction.svg\" /> \n",
    "</p>\n",
    "\n",
    "<!-- ![Picture](assets/images/house_pricing_prediction.svg){ width=\"800\" height=\"600\" style=\"display: block; margin: 0 auto\" } -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Logistic Regression\n",
    "* In classification, your output $y$ can take on only one of a small handful of possible values instead of any number in an infinite range of numbers.\n",
    "* It turns out that linear regression is not a good algorithm for classification problems,  which leads us into a different algorithm called logistic regression, which is one of the most popular and most widely used learning algorithms.\n",
    "* <span style=\"color:orange\"> Binary classification</span> is the type of classification where there are only two possible outputs.\n",
    "* The dividing line between the two possible outcomes is also called the <span style=\"color:orange\"> decision-boundary</span>.\n",
    "* Predictions of a binary type classification model should be between 0 and 1 since the output target is either 0 or 1. This can be accomplished by using a <span style=\"color:orange\">sigmoid function</span>  which maps all input values within a range of $(0, 1)$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    sig(z)=\\frac{1}{1+e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\hat{y}=sig(\\bar{w}.\\bar{x}+b)=\\frac{1}{1+e^{-(\\bar{w}.\\bar{x}+b)}}\n",
    "\\end{equation}\n",
    "\n",
    "* A comparison of linear regression and logistic regression with multiple features ($w$ is a vector)\n",
    "\n",
    "| linear regression | logistic regression|\n",
    "| :-----------: |:-----------|\n",
    "| $\\hat{y}=\\bar{w}.\\bar{x}+b$| $\\hat{y}=\\frac{1}{1+e^{-(\\bar{w}.\\bar{x}+b)}}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Logistic Loss Function\n",
    "* Remember that the cost function gives you a way to measure how well a specific set of parameters fits the training data.\n",
    "* Squared error cost function is NOT an ideal cost function for logistic regression since you can NOT get a convex cost function, which corresponds to a cost function with global minimum. In other words, if you were to try to use the squared error in gradient descent then there would be lots of local minima that you can get stuck.\n",
    "* There will be a different cost function that can make the cost function convex again where the gradient descent can be guaranteed to converge to the global minimum.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "L(\\hat{y_{i}},y_{i}) = \n",
    "\\left\\{\n",
    "    \\begin{array}{lr}\n",
    "        -log(\\hat{y_{i}}), & \\text{if  } y_{i} = 1\\\\\n",
    "        -log(1-\\hat{y_{i}}), & \\text{if  } y_{i} = 0\n",
    "    \\end{array}\n",
    "\\right\\} \n",
    "\\end{equation}\n",
    "\n",
    "* The log based loss function $L$ heavily penalizes the wrong result. For instance if the estimated output is $\\hat{y_{i}}=1$ while the target value is $y_{i} = 1$ then the loss penalty is zero, $(0)$ . On the other hand  if the estimated output $\\hat{y_{i}}=0$ while the target value is $y_{i} = 1$ then the penalty is $(\\infty)$.\n",
    " * A more compact version of the loss function is as follows,\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    L(\\hat{y_{i}},y_{i}) = -y_{i}log(\\hat{y_{i}}) -(1-y_{i})log(1-\\hat{y_{i}})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    J(\\bar{w},b)=\\frac{1}{m}\\sum\\limits_{i=1}^m L(\\hat{y_{i}},y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "* Applying partial derivatives to log-loss based cost function, we obtain a result quite similar to linear regression case except for the estimated output expression, $\\hat{y_i}$.\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\frac{\\partial{J(w,b)}}{\\partial{w}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (\\hat{y_i}-y_i)x_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\frac{\\partial{J(w,b)}}{\\partial{b}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (\\hat{y_i}-y_i)\n",
    "\\end{equation}\n",
    "\n",
    "* Remember the fact that the estimated output expression in logistic regression is different than the one in linear regression. Thus we can still use the same code to large extent.\n",
    "\n",
    "| linear regression | logistic regression|\n",
    "| :-----------: |:-----------|\n",
    "| $\\hat{y}=\\bar{w}.\\bar{x}+b$| $\\hat{y}=\\frac{1}{1+e^{-(\\bar{w}.\\bar{x}+b)}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Overfitting Issue\n",
    "* Both linear regression and logistic regression work well for many tasks. But sometimes in an application, the algorithm can run into a problem called overfitting, which can cause it to perform poorly.\n",
    "* <span style=\"color:orange\">Underfitting (high-bias)</span> occurs when the algorithm does not fit to the training data very well. We are biased that the data will fit into a specific  model (most possibly a simpler model). There's a clear pattern in the training data that the algorithm is just unable to capture. Another way to think of this form of bias is as if the learning algorithm has a very strong preconception, or we say a very strong bias.\n",
    "* <span style=\"color:orange\">Overfitting (high-variance)</span> occurs when the model fits the training set very well, it has fit the data almost too well, hence is overfit. The model is NOT able to generalize to new examples that's never seen before.\n",
    "*The intuition behind overfitting or high-variance is that the algorithm is trying very hard to fit every single training example. It turns out that if your training set were just even a little bit different,say, in a house prediction problem, one house was priced just a little bit more little bit less, then the function that the algorithm fits could end up being totally different.\n",
    "* For adressing the overfitting issue.\n",
    "    * First you can collect <span style=\"color:orange\">more training data</span>.\n",
    "    * Second option is <span style=\"color:orange\">feature selection</span>, to see if you can use fewer features. One disadvantage of feature selection is that by using only a subset of the features, the algorithm is throwing away some of the information that you have.\n",
    "    * Third option is <span style=\"color:orange\">regularization</span>, where you shrink the values of the model parameters without necessarily demanding that the parameter is set to exactly 0. It turns out that you end up with a curve fitting the training data much better.\n",
    "* Need figures for both regression and classification !!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Regularization\n",
    "* The intuition behind the regularization is to avoid the disadvantage of the feature selection concept. In feature selection, you may get rid of the fatures causing the high-variance by setting their weights to zero. However you would be also erasing the information coming from theses features. But you may just force the weight getting closer to zero instead of making them exactly zero.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (\\hat{y}^i-y^i)^2 +\\frac{\\lambda}{2m}\\sum\\limits_{i=1}^n (w_j)^2\n",
    "\\end{equation}\n",
    "\n",
    "* The $w_j$ corresponds multiple feature case where the $(j)$ index ranges within $(1,n)$\n",
    "* By choosing a large $(\\lambda)$ value, you may force $(w_j)$ values to get closer to zero, which will be a remedy for the overfitting issue.\n",
    "* Although the cost expression for linear and logistic regression are different, they will take the same form for the use in gradient descent except for the explicit expression of the estimated output, $\\hat{y}$.\n",
    "* Regularized cost function for the linear regression.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (\\bar{w}.\\bar{x}_i+b-y_i)^2 +\\frac{\\lambda}{2m}\\sum\\limits_{i=1}^n (w_j)^2\n",
    "\\end{equation}\n",
    "\n",
    "* Regularized cost function for the logistic regression.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    J(\\bar{w},b)=\\frac{1}{m}\\sum\\limits_{i=1}^m [-y_{i}log(\\hat{y_{i}}) -(1-y_{i})log(1-\\hat{y_{i}})] +\\frac{\\lambda}{2m}\\sum\\limits_{i=1}^n (w_j)^2\n",
    "    \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "\\hat{y_i}=\\frac{1}{1+e^{-(\\bar{w}.\\bar{x_i}+b)}}\n",
    "\\end{equation}\n",
    "\n",
    "* The partial gradients for both linear and logistic regression including the effect of the regularization. Although they are implicitly the same equation, remember that the explicite expression of $\\bar{y_i}$ is different.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\frac{\\partial{J(w,b)}}{\\partial{w_j}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (\\hat{y_i}-y_i)x_i +\\frac{\\lambda}{m}w_j\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\frac{\\partial{J(w,b)}}{\\partial{b}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (\\hat{y_i}-y_i)\n",
    "\\end{equation}\n",
    "* The next values of weight and biases can be calculates as follows, as the gradient descent algorithm works.\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    w_{k+1,j}=w_{k,j}-\\alpha\\frac{\\partial{J}}{\\partial{w_j}}|_{(w_{k,j},b_{k,j})}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    b_{k+1}=b_{k}-\\alpha\\frac{\\partial{J}}{\\partial{b}}|_{(w_{k,j},b_{k,j})}\n",
    "\\end{equation}\n",
    "* **Important Note:** The $(i)$ subscript ranges within $(1,m)$ corresponding to different items of training dataset, the $(j)$ corresponds to multiple feature capability of the model and ranges within $(1,n)$, and the $(k)$ subscript corresponds to iterations as the gradient descent algorithm advances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Unsupervised Learning\n",
    "* While the data comes with both inputs (x) and output labels (y) in supervised learning, the data comes <span style=\"color:orange\"> only with inputs (x)</span> in unsupervised learning  and the learning algorithm has to find some structure, some pattern or <span style=\"color:orange\">something interesting in the data.</span>\n",
    "* After supervised learning, the most widely used form of machine learning is unsupervised learning.\n",
    "* Assume that a dataset is given with patient's tumor size and the patient's age but not whether the tumor is benign or malignant.  Our job is to find some structure or some pattern or just find something interesting in the data. This is unsupervised learning, we call it unsupervised because we're not trying to supervise the algorithm.\n",
    "\n",
    "<p>\n",
    "    <img width=\"48%\" alt=\"peaks_3d\" src=\"assets/images/unsup_vs_sup_learning1.svg\"/> \n",
    "    <img width=\"48%\" alt=\"peaks_contour\" src=\"assets/images/unsup_vs_sup_learning2.svg\"/> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Clustering\n",
    "* The clustering algorithm is a type of unsupervised learning algorithm, which takes data without labels and tries to automatically group them into clusters.\n",
    "* A clustering algorithm groups similar data points together\n",
    "    * Given a database of customer data, automatically discover market segments and group customers into different market segments.\n",
    "    * Given a set of news articles found on the web, group them into sets of articles about the same stories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
