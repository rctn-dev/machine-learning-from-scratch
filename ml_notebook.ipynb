{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING FROM SCRATCH    \n",
    "* Machine learning is the field of study that gives computers the ability to <span style=\"color:orange\">learn without being explicitly programmmed</span>.  \n",
    "* Two main types of machine learning are <span style=\"color:orange\">supervised learning</span> and <span style=\"color:orange\">unsupervised learning</span>.  \n",
    "* Of these two, the supervised learning is the type of machine learning that is used most in many real-world applications and has seen the most rapid advancements and innovation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised Learning  \n",
    "* The 99% of the economic value created by machine learning today is through one type of machine learning, which is called supervised learning.\n",
    "* Supervised learning maps input (x) to output (y), where the learning algorithm learns from the <span style=\"color:orange\">right</span> answers.\n",
    "* Key characteristic of supervised learning is that you give your learning algorithm examples to learn from, called <span style=\"color:orange\">training set</span>.\n",
    "* The two major types of supervised learning are <span style=\"color:orange\">regression</span> and <span style=\"color:orange\">classification</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Regression\n",
    "* In a regression application, the learning algorithm has to predict numbers from within <span style=\"color:orange\">infinitely-many options</span>. In other words, it is used to <span style=\"color:orange\">predict continuous values</span>. To clarify, assume you have a prediction function which might pick any real number within the real number range of (0,1) as an output result such as 0.19, 0.73456, or 0.2717876756.\n",
    "    * The House price prediction based on size and many other attributes is a typical example of these kinds of problems.\n",
    "* The ultimate goal of the regression algorithm is to plot a  <span style=\"color:orange\">best-fit-line</span> or a <span style=\"color:orange\">best-fit-curve</span> between the data.\n",
    "* There are right answers, i.e., labels (values), within the given dataset since it is still supervised learning.\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"50%\" alt=\"house price prediction\" src=\"assets/images/house_pricing_prediction.svg\"/> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Linear Regression\n",
    "* Linear regression model means fitting a straight line to your data, which is probably the most widely used learning algorithm in the world today.\n",
    "*  The goal is to train the model $f$, for which you feed the training dataset, both the input features ($x$) and the output targets ($y$), to your learning algorithm. Then your supervised learning algorithm will produce some function (model) of $f$. The job of $f$ is to take a new input of $x$ and output an estimate (prediction) of $\\hat{y}$. \n",
    "* E.g., the linear regression model with one variable (single feature of $x$; i.e., $x$ should be a vector for representing multiple features of $x_{1}$, $x_{2}$, $x_{3}$, ... )\n",
    "\n",
    "    $\\hat{y}=f_{w,b}(x)=f(w,b,x)=wx+b$ \n",
    "    \n",
    "* Some conventional terms utilized while discussing a model.  \n",
    "\n",
    "\n",
    "| Term      | Description| Example\n",
    "| :-----------: |:-----------|:-------------|\n",
    "|$x$| input features|size of a house in the training dataset\n",
    "|$y$| output targets (right answers)| actual price of a house in the training dataset\n",
    "|$f$| model| house price prediction model for various sizes\n",
    "|$\\hat{y}$| predicted (estimated) output| the predicted price of a house \n",
    "|$(x^{i},y^{i})$| $i^{th}$ training sample| size and actual price of $i^{th}$ house in the training-set\n",
    "|$m$| number of training samples in the dataset|\n",
    "|$w$| model parameter: weight|\n",
    "|$b$| model  parameter: bias|\n",
    "|$J(w,b)$| total cost coming from all training dataset samples for each $(w,b)$ pair\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Cost Function\n",
    "\n",
    "* The model can not exactly match for all actual output targets (right answers) simultaneously, there should be an ensemble error (cost). E.g. for the $i^{th}$ sample of the training dataset, $\\hat{y}^{i} \\neq y^{i}$, still the expected and the actual output might be equal for some samples. However, the total error (cost) ($J$) is the important concept here. The cost function for linear model with single feature case is as follows, which is normalized with ($2m$) for preventing the cost function from diverging as the size of the dataset ($m$) increases.\n",
    "\n",
    "$J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (\\hat{y}^i-y^i)^2$, for a generic case. \n",
    "    \n",
    "$J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (w{x}^i+b-y^i)^2$, for the specific case where a linear model with single feature is utilized.\n",
    "\n",
    "* The goal of the learning algorithm (e.g. the gradient descent algorithm) is to minimize the cost function. \n",
    "* A graphical representation of how gradient descent advances with iterations of $(w,b)$ for finding the local minima of the cost function $J$.\n",
    "* Different initial points for the gradient descent algorithm might result in different local minima.\n",
    "<p>\n",
    "    <img width=\"40%\" alt=\"cost_3d\" src=\"assets/images/cost_3d.png\"/> \n",
    "    <img width=\"40%\" alt=\"cost_contour\" src=\"assets/images/cost_contour.svg\"/> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Gradient Descent\n",
    "* For a single feature linear model, $J$ is just a scalar value dependent on weight ($w$) and bias ($b$) scalar unknowns. These scalars are unknown for now since the learning algorithm is not trained yet with training data ($x^{i}, y^{i}$), which are known to us.\n",
    "* While ($w$) and ($b$) can be determined explicitely for simple cost ($J$) functions by simply examining the derivative,i.e., making it zero, it might not be possible for more complex cost functions.\n",
    "* Automate the process of optimizing $w$ and $b$ using gradient descent.\n",
    "* The gradient descent algorithm offers a numerical solution for automatically finding the minimum of complex cost functions as follows.\n",
    "    1. Choose a random starting point for weight and bias values, i.e., ($w^k, b^k$).\n",
    "    2. Consider that the solution set of ($w,b$) forming a vector space, in other words, each ($w,b$) item is a vector, and find the vector direction of maximum increase in cost function $J$, which describes a well-known concept in Calculus, which is the directional derivative of a scalar function.\n",
    "    3. Since we are looking for maximum descent instead of an ascent, put a minus sign in front of the $\\nabla$ operator and estimate the next values of  ($w^{k+1}, b^{k+1}$) with some step values $\\alpha$, which is called the learning rate until a convergence condition is satisfied.\n",
    "\n",
    "$\\theta^{k+1}=\\theta^{k}-\\alpha\\nabla{J}$\n",
    "* For $\\theta^{k}=(w^k,b^k)$, the gradient and iterative calculations are performed as follows, both of which should be updated simultaneously for each iteration.\n",
    "<!-- \n",
    "$\\nabla{J}=\\frac{\\partial{J}}{\\partial{w}}\\hat{w}+\\frac{\\partial{J}}{\\partial{b}}\\hat{b}$ -->\n",
    "$w^{k+1}=w^{k}-\\alpha\\frac{\\partial{J}}{\\partial{w}}|_{(w^k,b^k)}$\n",
    "\n",
    "$b^{k+1}=b^{k}-\\alpha\\frac{\\partial{J}}{\\partial{b}}|_{(w^k,b^k)}$\n",
    "\n",
    "\n",
    "* The expressions with $\\hat{w}$ and $\\hat{b}$ correspond to unit vector directions. \n",
    "Remembering the fact that matrices can also be considered as shorthands of vector repreentations, the following expression is also employed.\n",
    "\n",
    "<!-- $\\left[\\begin{array}{c}\n",
    "\\frac{\\partial{J}}{\\partial{w}} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial{J}}{\\partial{b}} \n",
    "\\end{array}\\right]$  -->\n",
    "\n",
    "* Applying partial derivatives for linear regression case.\n",
    "\n",
    "$J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (w{x}^i+b-y^i)^2$\n",
    "\n",
    "$\\frac{\\partial{J(w,b)}}{\\partial{w}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (w{x}^i+b-y^i)x^i$\n",
    "\n",
    "$\\frac{\\partial{J(w,b)}}{\\partial{b}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (w{x}^i+b-y^i)$\n",
    "* The complete form of the gradient for linear regression case, which will NOT be used in the coding part, we will decompose it to simpler code segments for **single responsibility** purposes. \n",
    "\n",
    "$w^{k+1}=w^{k}-\\frac{\\alpha}{m}\\sum\\limits_{i=1}^m (w{x}^i+b-y^i)x^i$\n",
    "\n",
    "$b^{k+1}=b^{k}-\\frac{\\alpha}{m}\\sum\\limits_{i=1}^m (w{x}^i+b-y^i)$\n",
    "\n",
    "*  The implementation of gradient algorithm can be decomposed into smaller code segments such as *compute_cost* for the computation of $J$ and *compute_partial_gradients* for the computation of partial derivatives at each iteration and finally for obtaining the actual result with *compute_gradient_descent*. The algorithm should advance as follows:\n",
    "    * Start with a random initial value for the weight and bias values as $(w^{k}, b^{k})$.\n",
    "    * Calculate the cost at $k$, $J^{k}$ as the sum of error $(\\hat{y}-y)^{2}$.\n",
    "    * Calculate the next values of $(w^{k+1}, b^{k+1})$.\n",
    "    * Calculate the new cost at  $J^{k+1}$.\n",
    "    * Test the convergence, e.g. if  $J^{k+1}>J^{k+1}$ then stop, else update the $(w^{k}, b^{k})$ with $(w^{k+1}, b^{k+1})$ and continue the loop.  \n",
    "\n",
    "* The implementation code for single feature linear regression is as follows, where $w$ is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for linear regression with single-feature\n",
    "import numpy as np\n",
    "def compute_cost(x_train,y_train,w,b):\n",
    "    ''' \n",
    "    Args:\n",
    "        x_train (np.array (m,)): training dataset, input features\n",
    "        y_train (np.array (m,)): training dataset, target values\n",
    "        w,b (scalar):model parameters\n",
    "    Return:\n",
    "        norm_cost (scalar): normalized total cost value.\n",
    "    '''\n",
    "    _cost=0\n",
    "    m=x_train.shape[0] # ndarray.shape outputs a tuple, e.g., (m,n) for a 2D matrix.\n",
    "    for i in range(m):\n",
    "        y_predict=w*x_train[i]+b\n",
    "        _cost+=(y_predict-y_train[i])**2\n",
    "    cost=_cost/2/m\n",
    "    return cost\n",
    "\n",
    "def compute_partial_gradients(x_train, y_train,w,b):\n",
    "    ''' \n",
    "    Args:\n",
    "        x_train (np.array (m,)): training dataset, input features\n",
    "        y_train (np.array (m,)): training dataset, target values\n",
    "        w,b (scalar):model parameters\n",
    "    Return:\n",
    "        dj_dw(scalar): partial gradient of the cost w.r.t the model parameter of w.\n",
    "        dj_db(scalar): partial gradient of the cost w.r.t the model parameter of b.\n",
    "    '''\n",
    "    _dj_dw=0\n",
    "    _dj_db=0\n",
    "    m=x_train.shape[0]\n",
    "    for i in range(m):\n",
    "        _dj_dw+=(w*x_train[i]+b-y_train[i])*x_train[i]\n",
    "        _dj_db+=(w*x_train[i]+b-y_train[i])\n",
    "    dj_dw,dj_db=_dj_dw/m,_dj_db/m\n",
    "    return (dj_dw,dj_db)\n",
    "\n",
    "def compute_gradient_descent(x_train,y_train, w_init,b_init,alpha,compute_cost,compute_partial_gradients):\n",
    "    ''' \n",
    "    Args:\n",
    "        x_train (np.array (m,)): training dataset, input features\n",
    "        y_train (np.array (m,)): training dataset, target values\n",
    "    Return:\n",
    "        w,b (scalar tuple): optimized model parameters.\n",
    "        cost_history (np.array): array of calculated costs while minimizing the cost function.\n",
    "        wb_history (np.array of tuples): array of caculated $(w,b)$ pairs while searching for optimum values.\n",
    "    '''\n",
    "    w_cur=w_init\n",
    "    b_cur=b_init\n",
    "    cost_next=1\n",
    "    cost_cur=0\n",
    "    cost_history=np.array([])\n",
    "    wb_history=np.array([])\n",
    "    while(cost_next<cost_cur):\n",
    "        cost_cur=compute_cost(x_train,y_train,w_cur,b_cur)\n",
    "        np.append(cost_history,cost_cur)\n",
    "        np.append(wb_history,(w_cur,b_cur))\n",
    "        (dj_dw,dj_db)=compute_partial_gradients(x_train, y_train,w_cur,b_cur)\n",
    "        w_next=w_cur-alpha*dj_dw\n",
    "        b_next=b_cur-alpha*dj_db\n",
    "        cost_next=compute_cost(x_train,y_train,w_next,b_next)\n",
    "        w_cur=w_next\n",
    "        b_cur=b_next\n",
    "    return (w_cur,b_cur), cost_history, wb_history\n",
    "\n",
    "w_init=0\n",
    "b_init=0\n",
    "alpha=1e-2\n",
    "x_train,y_train=load_data()\n",
    "w_final,b_final,cost_history,wb_history=compute_gradient_descent(x_train,y_train, w_init,b_init,alpha,compute_cost,compute_partial_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Multiple Linear Regression - Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Improving the Gradient Descent\n",
    "\n",
    "* The choice of the learning rate, $\\alpha$ will have a huge impact on the efficiency of the implementation of gradient descent.\n",
    "    * If the learning rate is too small, then gradient descents will work, but it will be slow.\n",
    "    * If the learning rate is too large, the intersect may overshoot and may never reach the minimum, may fail to converge and may even diverge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Classification \n",
    "* While the regression type learning tries to predict an output among infinitely-many options, the classification type tries to predict the output, i.e. classes or categories, within only a small number, i.e., <span style=\"color:orange\">finitely-many options</span>. \n",
    "* The ultimate goal of the classification algorithm is to find a <span style=\"color:orange\"> boundary-line </span> or a <span style=\"color:orange\">boundary-curve</span> that seperates actual outputs.\n",
    "* <span style=\"color:orange\">Categories (classes)</span> can be <span style=\"color:orange\">numeric</span> like 0, 1 or 0, 1, 2. Neverthelesss, they don't have to be numbers, they can be <span style=\"color:orange\">non-numeric</span>. To illustrate, the classification can predict whether a picture is that of a cat or a dog or it can predict if a tumor is benign or malignant.\n",
    "* There are right answers,i.e., labels, within the given dataset since it is still supervised learning. \n",
    "* Some examples of classification can be as follows;  \n",
    "    * Given email labeled as spam/not spam, design a spam filter.\n",
    "    * Given a dataset of patients diagnosed as either having diabetes or not, learn to classify new patients as having diabetes or not.\n",
    "    * Given pictures of dogs, cats, and wolves, learn to classify if a new picture is that of a dog, cat, or wolf.\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"50%\" alt=\"tumor prediction\" src=\"assets/images/tumor_prediction.svg\" /> \n",
    "</p>\n",
    "\n",
    "<!-- ![Picture](assets/images/house_pricing_prediction.svg){ width=\"800\" height=\"600\" style=\"display: block; margin: 0 auto\" } -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1 Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unsupervised Learning\n",
    "* While the data comes with both inputs (x) and output labels (y) in supervised learning, the data comes <span style=\"color:orange\"> only with inputs (x)</span> in unsupervised learning  and the learning algorithm has to find some structure, some pattern or <span style=\"color:orange\">something interesting in the data.</span>\n",
    "* After supervised learning, the most widely used form of machine learning is unsupervised learning.\n",
    "* Assume that a dataset is given with patient's tumor size and the patient's age but not whether the tumor is benign or malignant.  Our job is to find some structure or some pattern or just find something interesting in the data. This is unsupervised learning, we call it unsupervised because we're not trying to supervise the algorithm.\n",
    "\n",
    "<p>\n",
    "    <img width=\"48%\" alt=\"peaks_3d\" src=\"assets/images/unsup_vs_sup_learning1.svg\"/> \n",
    "    <img width=\"48%\" alt=\"peaks_contour\" src=\"assets/images/unsup_vs_sup_learning2.svg\"/> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Clustering\n",
    "* The clustering algorithm is a type of unsupervised learning algorithm, which takes data without labels and tries to automatically group them into clusters.\n",
    "* A clustering algorithm groups similar data points together\n",
    "    * Given a database of customer data, automatically discover market segments and group customers into different market segments.\n",
    "    * Given a set of news articles found on the web, group them into sets of articles about the same stories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
