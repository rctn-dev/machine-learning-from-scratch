{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING FROM SCRATCH    \n",
    "* Machine learning is the field of study that gives computers the ability to <span style=\"color:orange\">learn without being explicitly programmmed</span>.  \n",
    "* Two main types of machine learning are <span style=\"color:orange\">supervised learning</span> and <span style=\"color:orange\">unsupervised learning</span>.  \n",
    "* Of these two, the supervised learning is the type of machine learning that is used most in many real-world applications and has seen the most rapid advancements and innovation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised Learning  \n",
    "* The 99% of the economic value created by machine learning today is through one type of machine learning, which is called supervised learning.\n",
    "* Supervised learning maps input (x) to output (y), where the learning algorithm learns from the <span style=\"color:orange\">right</span> answers.\n",
    "* Key characteristic of supervised learning is that you give your learning algorithm examples to learn from, called <span style=\"color:orange\">training set</span>.\n",
    "* The two major types of supervised learning are <span style=\"color:orange\">regression</span> and <span style=\"color:orange\">classification</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Regression\n",
    "* In a regression application, the learning algorithm has to predict numbers from within <span style=\"color:orange\">infinitely-many options</span>. In other words, it is used to <span style=\"color:orange\">predict continuous values</span>. To clarify, assume you have a prediction function which might pick any real number within the real number range of (0,1) as an output result such as 0.19, 0.73456, or 0.2717876756.\n",
    "    * The House price prediction based on size and many other attributes is a typical example of these kinds of problems.\n",
    "* The ultimate goal of the regression algorithm is to plot a  <span style=\"color:orange\">best-fit-line</span> or a <span style=\"color:orange\">best-fit-curve</span> between the data.\n",
    "* There are right answers, i.e., labels (values), within the given dataset since it is still supervised learning.\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"50%\" alt=\"house price prediction\" src=\"assets/images/house_pricing_prediction.svg\"/> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Linear Regression\n",
    "* Linear regression model means fitting a straight line to your data, which is probably the most widely used learning algorithm in the world today.\n",
    "*  The goal is to train the model $f$, for which you feed the training dataset, both the input features ($x$) and the output targets ($y$), to your learning algorithm. Then your supervised learning algorithm will produce some function (model) of $f$. The job of $f$ is to take a new input of $x$ and output an estimate (prediction) of $\\hat{y}$. \n",
    "* E.g., the linear regression model with one variable (single feature of $x$; i.e., $x$ should be a vector for representing multiple features of $x_{1}$, $x_{2}$, $x_{3}$, ... )\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\hat{y}=f_{w,b}(x)=f(w,b,x)=wx+b\n",
    "\\end{equation}\n",
    "    \n",
    "* Some conventional terms utilized while discussing a model.  \n",
    "\n",
    "\n",
    "| Term      | Description| Example\n",
    "| :-----------: |:-----------|:-------------|\n",
    "|$x$| input features|size of a house in the training dataset\n",
    "|$y$| output targets (right answers)| actual price of a house in the training dataset\n",
    "|$f$| model| house price prediction model for various sizes\n",
    "|$\\hat{y}$| predicted (estimated) output| the predicted price of a house \n",
    "|$(x_{i},y_{i})$| $i^{th}$ training sample| size and actual price of $i^{th}$ house in the training-set\n",
    "|$m$| number of training samples in the dataset|\n",
    "|$w$| model parameter: weight|\n",
    "|$b$| model  parameter: bias|\n",
    "|$J(w,b)$| total cost coming from all training dataset samples for each $(w,b)$ pair\n",
    "|$i$ subscript|$1<i<m$ corresponds to different items of training dataset \n",
    "|$j$ subscript|$1<j<n$ corresponds to different fatures of the model\n",
    "|$k$ subscript|$1<k< ? $ corresponds to iteration as gradient descent algo works\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Cost Function for Linear Regression\n",
    "\n",
    "* The model can not exactly match for all actual output targets (right answers) simultaneously, there should be an ensemble error (cost). E.g. for the $i^{th}$ sample of the training dataset, $\\hat{y}^{i} \\neq y^{i}$, still the expected and the actual output might be equal for some samples. However, the total error (cost) ($J$) is the important concept here. The cost function for linear model with single feature case is as follows, which is normalized with ($2m$) for preventing the cost function from diverging as the size of the dataset ($m$) increases.\n",
    "* For a generic case,\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (\\hat{y}_i-y_i)^2\n",
    "\\end{equation}\n",
    "* For the specific case where a linear model with single feature is utilized.\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (w{x}_i+b-y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "* The goal of the learning algorithm (e.g. the gradient descent algorithm) is to minimize the cost function. \n",
    "* A graphical representation of how gradient descent advances with iterations of $(w,b)$ for finding the local minima of the cost function $J$.\n",
    "* Different initial points for the gradient descent algorithm might result in different local minima.\n",
    "<p>\n",
    "    <img width=\"40%\" alt=\"cost_3d\" src=\"assets/images/cost_3d.png\"/> \n",
    "    <img width=\"40%\" alt=\"cost_contour\" src=\"assets/images/cost_contour.svg\"/> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:orange\"> Place a slider over w and b, showing both the cost and the fitting !!! </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Polynomial Regression\n",
    "* Take the ideas of multiple linear regression and feature engineering to come up with a new algorithm called polynomial regression, which will let you fit curves, non-linear functions, to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Gradient Descent Algorithm\n",
    "* For a single feature linear model, $J$ is just a scalar value dependent on weight ($w$) and bias ($b$) scalar unknowns. These scalars are unknown for now since the learning algorithm is not trained yet with training data ($x^{i}, y^{i}$), which are known to us.\n",
    "* While ($w$) and ($b$) can be determined explicitely for simple cost ($J$) functions by simply examining the derivative,i.e., making it zero, it might not be possible for more complex cost functions.\n",
    "* Automate the process of optimizing $w$ and $b$ using gradient descent.\n",
    "* The gradient descent algorithm offers a numerical solution for automatically finding the minimum of complex cost functions as follows.\n",
    "    1. Choose a random starting point for weight and bias values, i.e., ($w_k, b_k$).\n",
    "    2. Consider that the solution set of ($w,b$) forming a vector space, in other words, each ($w,b$) item is a vector, and find the vector direction of maximum increase in cost function $J$, which describes a well-known concept in Calculus, which is the directional derivative of a scalar function.\n",
    "    3. Since we are looking for maximum descent instead of an ascent, put a minus sign in front of the $\\nabla$ operator and estimate the next values of  ($w^{k+1}, b^{k+1}$) with some step values $\\alpha$, which is called the learning rate until a convergence condition is satisfied.\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\theta_{k+1}=\\theta_{k}-\\alpha\\nabla{J}\n",
    "\\end{equation}\n",
    "* For $\\theta_{k}=(w_k,b_k)$, the gradient and iterative calculations are performed as follows, both of which should be updated simultaneously for each iteration.\n",
    "<!-- \n",
    "$\\nabla{J}=\\frac{\\partial{J}}{\\partial{w}}\\hat{w}+\\frac{\\partial{J}}{\\partial{b}}\\hat{b}$ -->\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    w_{k+1}=w_{k}-\\alpha\\frac{\\partial{J}}{\\partial{w}}|_{(w_k,b_k)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    b_{k+1}=b_{k}-\\alpha\\frac{\\partial{J}}{\\partial{b}}|_{(w_k,b_k)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "* The expressions with $\\hat{w}$ and $\\hat{b}$ correspond to unit vector directions. \n",
    "Remembering the fact that matrices can also be considered as shorthands of vector repreentations, the following expression is also employed.\n",
    "\n",
    "<!-- $\\left[\\begin{array}{c}\n",
    "\\frac{\\partial{J}}{\\partial{w}} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial{J}}{\\partial{b}} \n",
    "\\end{array}\\right]$  -->\n",
    "\n",
    "## 3.1 Single Feature Linear Regression\n",
    "\n",
    "* Applying partial derivatives for linear regression case.\n",
    "<!-- \n",
    "$J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (w{x}^i+b-y^i)^2$ -->\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\frac{\\partial{J(w,b)}}{\\partial{w}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (w{x}_i+b-y_i)x_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\frac{\\partial{J(w,b)}}{\\partial{b}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (wx_i+b-y_i)\n",
    "\\end{equation}\n",
    "\n",
    "* The complete form of the gradient for linear regression case, which will NOT be used in the coding part, we will decompose it to simpler code segments for **single responsibility** purposes. \n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    w_{k+1}=w_{k}-\\frac{\\alpha}{m}\\sum\\limits_{i=1}^m (wx_i+b-y_i)x_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    b_{k+1}=b_{k}-\\frac{\\alpha}{m}\\sum\\limits_{i=1}^m (wx_i+b-y_i)\n",
    "\\end{equation}\n",
    "\n",
    "*  The implementation of gradient algorithm can be decomposed into smaller code segments such as *compute_cost* for the computation of $J$ and *compute_partial_gradients* for the computation of partial derivatives at each iteration and finally for obtaining the actual result with *compute_gradient_descent*. The algorithm should advance as follows:\n",
    "    * Start with a random initial value for the weight and bias values as $(w_{k}, b_{k})$.\n",
    "    * Calculate the cost at $k$, $J_{k}$ as the sum of error $(\\hat{y}_k-y)^{2}$.\n",
    "    * Calculate the next values of $(w_{k+1}, b_{k+1})$.\n",
    "    * Calculate the new cost at  $J_{k+1}$.\n",
    "    * Test the convergence, e.g. if  $J_{k+1}>J_{k+1}$ then stop, else update the $(w_{k}, b_{k})$ with $(w_{k+1}, b_{k+1})$ and continue the loop.  \n",
    "\n",
    "* The implementation code for single feature linear regression is as follows, where $w$ is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum (w,b): ( 199.9898, 100.0164)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "def compute_cost(x, y, w, b):\n",
    "    '''\n",
    "    Args:\n",
    "      x (np.array (m,)): training dataset, input features.\n",
    "      y (np.array (m,)): training dataset, target values.\n",
    "      w,b (scalar)     : model parameters for single feature linear regression, w is NOT vector.\n",
    "   \n",
    "    Return:\n",
    "      cost (scalar): total cost,i.e., square-error between the target and the estimated output. \n",
    "    '''\n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        y_hat = w * x[i] + b\n",
    "        cost = cost + (y_hat - y[i])**2\n",
    "    cost = 1/(2 * m) * cost\n",
    "    return cost\n",
    "\n",
    "def compute_partial_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the partial gradient for linear regression with single feature.\n",
    "    Args:\n",
    "      x (np.array (m,)): training dataset, input features.\n",
    "      y (np.array (m,)): training dataset, target values.\n",
    "      w,b (scalar)     : model parameters for single feature linear regression, w is NOT vector.\n",
    "    Return:\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    for i in range(m):  \n",
    "        y_hat=w*x[i]+b \n",
    "        dj_dw+= (y_hat - y[i]) * x[i] \n",
    "        dj_db+= y_hat - y[i] \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def compute_gradient_descent(x, y, w_init, b_init, alpha, max_iters, compute_cost, compute_partial_gradient): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (np.array (m,))  :Training dataset, input features\n",
    "      y (np.array (m,))  :Training dataset, target values\n",
    "      w_init,b_init (scalar): initial values of model parameters  \n",
    "      alpha (float):     Learning rate\n",
    "      max_iters (int):   number of iterations to run gradient descent\n",
    "      compute_cost:     function to call to produce cost\n",
    "      compute_partial_gradient: function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      cost_history (List): History of cost values\n",
    "      wb_history (list): History of parameters [w,b] \n",
    "      \"\"\"\n",
    "    cost_history = []\n",
    "    wb_history = []\n",
    "    b = b_init\n",
    "    w = w_init\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        dj_dw, dj_db = compute_partial_gradient(x, y, w , b)     \n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "        cost_history.append( compute_cost(x, y, w , b))\n",
    "        wb_history.append([w,b]) \n",
    "    return w, b, cost_history, wb_history \n",
    "\n",
    "# initialize parameters\n",
    "w_init = 0.1\n",
    "b_init = 10\n",
    "max_iters = 10000\n",
    "alpha = 1e-2\n",
    "x_train=np.array([1,2])\n",
    "y_train=np.array([300,500])\n",
    "\n",
    "# Second dataset is problematic, why???\n",
    "# x_train=np.array([480,600,750,900,1000,1200,1500,1600,1800,2200])\n",
    "# y_train=np.array([85,120,200,150,250,230,280,260,290,310])\n",
    "\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = compute_gradient_descent(x_train ,y_train, w_init, b_init, alpha, \n",
    "                                                    max_iters, compute_cost, compute_partial_gradient)\n",
    "print(f\"Optimum (w,b): ({w_final:9.4f},{b_final:9.4f})\")\n",
    "plt.plot(J_hist[1000:])\n",
    "plt.plot(J_hist[1000:])\n",
    "plt.xlabel(\"# of iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Multiple Feature Linear Regression - Vectorization\n",
    "* The multiple linear regression is probably the single most widely used learning algorithm in the world today.\n",
    "* For a typical multiple feature linear regression case, the estimated value is expressed as follows, where $x_{1}, x_{2}, x_{3}, ...,  x_{n}$ correspond for different features.\n",
    "\n",
    "| single-feature| multiple-feature|\n",
    "| :-----------: |:-----------|\n",
    "| $\\hat{y}=wx+b$| $\\hat{y}=w_{1} x_{1}+w_{2} x_{2}+w_{3} x_{3}+...+x_{n}+b$ \n",
    "\n",
    "* It is better to implement matrix formulation for a better picturing of the estimated output of multiple feature case and rationalize the vectorization.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\hat{y}=w_{1} x_{1}+w_{2} x_{2}+w_{3} x_{3}+...+x_{n}+b = \\bar{w}.\\bar{x}+b\n",
    "\\end{equation}\n",
    "\n",
    "*  where the bar sign corresponds for the vector version of the weights and features.\n",
    "* Vectorization has benefits when you're implementing a learning algorithm, such as:\n",
    "    * The code will be <span style=\"color:orange\">shorter</span>.\n",
    "    * The code will run <span style=\"color:orange\">faster</span>, much more efficiently.\n",
    "* Vectorized code will allow you to also take advantage of modern numerical linear algebra libraries, as well as maybe even GPU hardware, which is originally designed to speed up computer graphics in your computer, but it turns out that it can help you execute your code much more quickly when you write vectorized code.\n",
    "* Numpy is a numerical linear algebra library in Python, which is by far the most widely used numerical linear algebra library in Python and in machine learning.\n",
    "* NumPy dot function is a vectorized implementation of the dot product operation between two vectors and especially when n is large, which will run much faster.\n",
    "* The NumPy dot function is able to use parallel hardware in your computer and this is true whether you're running this on a <span style=\"color:orange\">normal-CPU </span> or on a <span style=\"color:orange\">GPU</span>.\n",
    "* Vectorized implementations of learning algorithms has been a key step to getting learning algorithms to run efficiently, and therefore <span style=\"color:orange\">scale-up</span> well to large datasets that many modern machine learning algorithms now have to operate on.\n",
    "* For each training dataset and for a feature size of $(n)$, the estimated output belonging to the $i^{th}$ training dataset.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\hat{y_i}=w_{1} x_{i1}+w_{2} x_{i2}+...+x_{in}+b =\\bar{x_i}.\\bar{w}+b\n",
    "\\end{equation}\n",
    "\n",
    "* For a multiple feature size of $n$ and a training dataset size of $m$, the vectorization should be as follows.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\hat{y_1}=w_{1} x_{11}+w_{2} x_{12}+...+x_{1n}+b\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\hat{y_2}=w_{1} x_{21}+w_{2} x_{22}+...+x_{2n}+b \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    ...\n",
    "\\end{equation} \n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\hat{y}_m=w_{1} x_{m1}+w_{2} x_{m2}+...+x_{mn}+b \n",
    "\\end{equation}\n",
    "* The vectorized version, the same $(b)$ value is added since it is still just a scalar.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "\\left[\\begin{array}{c}\n",
    "\\hat{y}_1\\\\\n",
    "\\\\\n",
    "\\hat{y}_2\\\\ \n",
    "\\\\\n",
    "...\n",
    "\\\\\n",
    "\\hat{y}_m\\\\ \n",
    "\\end{array}\\right]=\\left[\\begin{array}{c c c c}\n",
    "x_{11} & x_{12} & ... & x_{1n} \\\\\n",
    "\\\\\n",
    "x_{21} & x_{22} & ... & x_{2n} \\\\\n",
    "\\\\\n",
    "... & ... & ... & ... \\\\\n",
    "\\\\\n",
    "x_{m1} & x_{m2} & ... & x_{mn} \\\\\n",
    "\\end{array}\\right].\\left[\\begin{array}{c}\n",
    "w_1\\\\\n",
    "\\\\\n",
    "w_2\\\\ \n",
    "\\\\\n",
    "...\n",
    "\\\\\n",
    "w_m\\\\ \n",
    "\\end{array}\\right]+ \\left[\\begin{array}{c}\n",
    "b\\\\\n",
    "\\\\\n",
    "b\\\\ \n",
    "\\\\\n",
    "...\n",
    "\\\\\n",
    "b\n",
    "\\end{array}\\right]\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cost function is similar to single feature case, except for the  vector notation, the estimated an dthe target values are vectors of size $(n)$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (\\bar{\\hat{y}_i}-\\bar{y_i})^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\bar{\\hat{y}_i}=\\bar{x_i}.\\bar{w_i}+b\n",
    "\\end{equation}\n",
    "\n",
    "* gradient matrix ? \n",
    "\n",
    "pass\n",
    "\n",
    "* step calculations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_norm,b_norm:[110.61335173 -21.47323884 -32.66070323 -37.77938362], 362.24\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.51849608  0.44436943 -0.78288136  0.05235597]\n",
      " predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = $318936\n",
      "219.711301764962\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def zscore_normalize(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x (np.array (m,n))     : training dataset inputs, m examples, n features\n",
    "    Returns:\n",
    "      x_norm (np.array (m,n)): input normalized by column,corresponding to each feature.\n",
    "      mu (np.array (n,))     : mean of each feature, i.e., of each column\n",
    "      sigma (np.array (n,))  : standard deviation of each feature,i.e., of each column\n",
    "    \"\"\"\n",
    "    # axis=0 means, take mean of each column (size of m) and create a row vector of means (size of n).\n",
    "    mu=np.mean(x,axis=0) # dim: (n,)\n",
    "    sigma=np.std(x,axis=0)# dim: (n,)               \n",
    "    x_norm=(x-mu)/sigma      \n",
    "    return (x_norm, mu, sigma)\n",
    "\n",
    "def compute_cost(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "      X (np.darray (m,n)): training datasets, m examples with n features\n",
    "      y (np.array (m,)) : target values\n",
    "      w (np.array (n,)) : model parameters, n features  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = x.shape[0]\n",
    "    cost = 0.0\n",
    "    y_hat=x@w+b # for matrix-vector multiplication, use np.dot or @ or np.matmul\n",
    "    #y_hat=np.dot(x,w)+b # use np.dot or @ or np.matmul\n",
    "    #y_hat=np.matmul(x,w)+b# use np.dot or @ or np.matmul\n",
    "    cost=(1/(2*m))*np.sum((y_hat-y)**2)\n",
    "    return cost\n",
    "\n",
    "def compute_partial_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (np.array (m,n)): Data, m examples with n features\n",
    "      y (np.array (m,)) : target values\n",
    "      w (np.array (n,)) : model parameters  \n",
    "      b (scalar)        : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (np.array (n,)): The gradient vector of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)       : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n=x.shape           #(number of examples, number of features)\n",
    "    dj_dw=np.zeros((n,))\n",
    "    dj_db=0.\n",
    "    y_hat=x@w+b\n",
    "    err=y_hat-y \n",
    "    # matrix operation makes the error sum autimatically for the dj_dw\n",
    "    dj_dw = (1/m)*(x.T@err)    \n",
    "    dj_db = (1/m)*np.sum(err)                                                                           \n",
    "    return dj_db, dj_dw\n",
    "\n",
    "def compute_gradient_descent(X, y, w_init, b_init,alpha, max_iters,compute_cost, compute_partial_gradient): \n",
    "    '''\n",
    "    Args:\n",
    "      X (np.array (m,n))   : Data, m examples with n features\n",
    "      y (np.array (m,))    : target values\n",
    "      w_init (ndarray (n,)) : initial model parameters  \n",
    "      b_init (scalar)       : initial model parameter\n",
    "      compute_cost       : compute cost\n",
    "      compute_partial_gradient   : compute the partial gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      max_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      '''\n",
    "    cost_history = []\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    for i in range(max_iters):\n",
    "        dj_db,dj_dw = compute_partial_gradient(X, y, w, b)\n",
    "        w = w - alpha * dj_dw        \n",
    "        b = b - alpha * dj_db              \n",
    "        cost_history.append( compute_cost(X, y, w, b))\n",
    "    return w, b, cost_history\n",
    "\n",
    "data = np.loadtxt(\"assets/data/houses.txt\",delimiter=',')  \n",
    "x_train,y_train=data[:,0:4], data[:,4]\n",
    "\n",
    "w_init=np.zeros(x_train.shape[1])\n",
    "b_init=0.\n",
    "max_iters=1000\n",
    "alpha=1.0e-1\n",
    "\n",
    "x_norm,x_mu,x_sigma=zscore_normalize(x_train)\n",
    "w_norm, b_norm,cost_history =compute_gradient_descent(x_norm, y_train, w_init, b_init,alpha, max_iters,compute_cost, compute_partial_gradient)\n",
    "print(f'w_norm,b_norm:{w_norm}, {b_norm:.2f}')\n",
    "plt.plot(cost_history)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x_house = np.array([1200, 3, 1, 40])\n",
    "# Normalize the input feature\n",
    "x_house_norm = (x_house - x_mu) / x_sigma\n",
    "print(x_house_norm)\n",
    "x_house_predict = np.dot(x_house_norm, w_norm) + b_norm\n",
    "print(f\" predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000:0.0f}\")\n",
    "print(cost_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Improving Gradient Descent\n",
    "\n",
    "## 4.1 Feature Scaling\n",
    "* Feature scaling will enable gradient descent to run much faster.\n",
    "* When you have different features that take on very different ranges of values, it can cause gradient descent to run slowly in order to prevent bouncing-back and forth but re-scaling the different features so they all take on comparable range of values can speed up gradient descent significantly. \n",
    "* The mean normalization and z-score normalization are two of various normalizationntechniques to re-scale the ranges.\n",
    "* The scaled features get very accurate results much, much faster!. The gradient of each parameter gets tiny by the end of a fairly short run. A learning rate of $(0.1)$ is a good start for regression with normalized features.\n",
    "* For the house data, before and after normalization, the feature distribution is as follows.\n",
    "figures\n",
    "* Another way to view feature scaling is in terms of the cost contours. When feature scales do not match, the plot of cost versus parameters in a contour plot is asymmetric.\n",
    "figures.\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"80%\" alt=\"house price prediction\" src=\"assets/images/original_feature_scaling.svg\"/> \n",
    "</p>\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"80%\" alt=\"house price prediction\" src=\"assets/images/norm_feature_scaling.svg\"/> \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x (np.array (m,n))     : training dataset inputs, m examples, n features\n",
    "    Returns:\n",
    "      x_norm (np.array (m,n)): input normalized by column,corresponding to each feature.\n",
    "      mu (np.array (n,))     : mean of each feature, i.e., of each column\n",
    "      sigma (np.array (n,))  : standard deviation of each feature,i.e., of each column\n",
    "    \"\"\"\n",
    "    # axis=0 means, take mean of each column (size of m) and create a row vector of means (size of n).\n",
    "    mu=np.mean(x,axis=0) # dim: (n,)\n",
    "    sigma=np.std(x,axis=0)# dim: (n,)               \n",
    "    x_norm=(x-mu)/sigma      \n",
    "    return (x_norm, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature Engineering and Polynomial Regression\n",
    "* The choice of features can have a huge impact on your learning algorithm's performance. In fact, for many practical applications, choosing or entering the right features is a critical step to making the algorithm work well.\n",
    "* Creating a new feature is an example of what's called feature engineering, in which you might use your knowledge or intuition about the problem to design new features usually by transforming or combining the original features of the problem in order to make it easier for the learning algorithm to make accurate predictions. \n",
    "* Feature engineering not only allows you to fit straight lines to your data, but also allows curves and non-linear functions too.\n",
    "\n",
    "<p>\n",
    "    <img width=\"30%\" alt=\"poly_reg\" src=\"assets/images/polynomial_regression.svg\"/> \n",
    "    <img width=\"30%\" alt=\"poly_reg_norm\" src=\"assets/images/polynomial_regression_norm.svg\"/> \n",
    "    <img width=\"30%\" alt=\"poly_reg_norm\" src=\"assets/images/sinx_regression.svg\"/> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Learning Rate\n",
    "* Learning algorithm will run much better with an appropriate choice of learning rate. \n",
    "    * If it's too small, it will run very slowlY \n",
    "    * If it is too large, it may not even converge.\n",
    "* If you plot the cost for a number of iterations and notice that the costs sometimes goes up and sometimes goes down, you should take that as a clear sign that gradient descent is not working properly.\n",
    "    * This could mean that there's a bug in the code. \n",
    "    * Or sometimes it could mean that your learning rate is too large.\n",
    "* Note that setting $\\alpha$ to be really small is meant here as a debugging step and a very small value of $\\alpha$ is not going to be the most efficient choice for actually training your learning algorithm.\n",
    "## 4.4 Convergence Condition\n",
    "* Number of iterations that gradient descent converges can vary a lot between different applications. In one application, it may converge after just 30 iterations. For a different application, it could take 1000 or 100,000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Classification \n",
    "* While the regression type learning tries to predict an output among infinitely-many options, the classification type tries to predict the output, i.e. classes or categories, within only a small number, i.e., <span style=\"color:orange\">finitely-many options</span>. \n",
    "* The ultimate goal of the classification algorithm is to find a <span style=\"color:orange\"> boundary-line </span> or a <span style=\"color:orange\">boundary-curve</span> that seperates actual outputs.\n",
    "* <span style=\"color:orange\">Categories (classes)</span> can be <span style=\"color:orange\">numeric</span> like 0, 1 or 0, 1, 2. Neverthelesss, they don't have to be numbers, they can be <span style=\"color:orange\">non-numeric</span>. To illustrate, the classification can predict whether a picture is that of a cat or a dog or it can predict if a tumor is benign or malignant.\n",
    "* There are right answers,i.e., labels, within the given dataset since it is still supervised learning. \n",
    "* Some examples of classification can be as follows;  \n",
    "    * Given email labeled as spam/not spam, design a spam filter.\n",
    "    * Given a dataset of patients diagnosed as either having diabetes or not, learn to classify new patients as having diabetes or not.\n",
    "    * Given pictures of dogs, cats, and wolves, learn to classify if a new picture is that of a dog, cat, or wolf.\n",
    "* <span style=\"color:orange\">single variate logistic regression ekle, aşağıdaki multivariate </span>\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"50%\" alt=\"tumor prediction\" src=\"assets/images/tumor_prediction.svg\" /> \n",
    "</p>\n",
    "\n",
    "* what happens when you try to use linear regression for classification on categorical data? -Linear regression is more erronous when compared to sigmoid-based regression when new data is introduced.\n",
    "\n",
    "*  <span style=\"color:orange\">Add figures :)</span>\n",
    "\n",
    "* It turns out that linear regression is not a good algorithm for classification problems,  which leads us into a different algorithm called logistic regression, which is one of the most popular and most widely used learning algorithms.\n",
    "\n",
    "<!-- ![Picture](assets/images/house_pricing_prediction.svg){ width=\"800\" height=\"600\" style=\"display: block; margin: 0 auto\" } -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Logistic Regression\n",
    "\n",
    "* <span style=\"color:orange\"> Binary classification</span> is the type of classification where there are only two possible outputs.\n",
    "* Logistic regression is one of the most popular algorithms for binary classification. Predictions of a binary type classification model should be between $(0)$ and $(1)$ since the output target is either $(0)$ or $(1)$. This can be accomplished by using a <span style=\"color:orange\">sigmoid function</span>  which maps all input values within a range of $(0, 1)$ as the output, which can be interpreted as the <span style=\"color:orange\">probabilities</span> of each example belonging to a particular class.\n",
    "* Logistic regression determines the best predicted weights $w_0, w_1,...,w_n$ such that the sigmoid function $sig(z)$ is as close as possible to all actual responses $y_i, i=1,2,...,m$, where $m$ is the number of observations and $n$ is the number of features.\n",
    "* In order to get the best weights, we usually maximize the log-likelihood function (LLF) for all observations, which is called the **maximum likelihood estimation**, which is bascially used for penalizing wrong outputs and rewarding the right outputs.\n",
    "* We need an understanding of both the sigmoid function and the <span style=\"color:orange\">natural logarithm</span> function to understand what logistic regression is and how it works. .\n",
    "* The dividing line between the two possible outcomes is called the <span style=\"color:orange\"> decision-boundary</span>.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    sig(z)=\\frac{1}{1+e^{-z}}\n",
    "\\end{equation}\n",
    "* Typical sigmoid function.\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"50%\" alt=\"tumor prediction\" src=\"assets/images/sigmoid_func.svg\" /> \n",
    "</p>\n",
    " \n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    z=\\bar{w}.\\bar{x}+b\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\hat{y}=sig(\\bar{w}.\\bar{x}+b)=\\frac{1}{1+e^{-(\\bar{w}.\\bar{x}+b)}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    LLF=\\sum\\limits_{i=1}^m [-y_{i}log(sig(z_{i})) -(1-y_{i})log(1-sig(z_{i}))] \n",
    "    \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "* A comparison of linear regression and logistic regression with multiple features ($w$ is a vector). The sigma function is essenatially a tranformation squeezing the estimation results into $(0,1)$ range since it makes more sense. The natural logarithm is also a specific cost function different than a square-error which specifically targets penalizing a sigmoid-like function.\n",
    "\n",
    "| linear regression | logistic regression|\n",
    "| :-----------: |:-----------|\n",
    "| $\\hat{y}=\\bar{w}.\\bar{x}+b$| $\\hat{y}=\\frac{1}{1+e^{-(\\bar{w}.\\bar{x}+b)}}$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Logistic-Loss Function\n",
    "* Remember that the cost function gives you a way to measure how well a specific set of parameters fits the training data.\n",
    "* Squared error cost function is NOT an ideal cost function for logistic regression since you can NOT get a convex cost function, which corresponds to a cost function with global minimum. In other words, if you were to try to use the squared error in gradient descent then there would be lots of local minima that you can get stuck.\n",
    "* There will be a different cost function that can make the cost function convex again where the gradient descent can be guaranteed to converge to the global minimum.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "L(\\hat{y_{i}},y_{i}) = \n",
    "\\left\\{\n",
    "    \\begin{array}{lr}\n",
    "        -log(\\hat{y_{i}}), & \\text{if  } y_{i} = 1\\\\\n",
    "        -log(1-\\hat{y_{i}}), & \\text{if  } y_{i} = 0\n",
    "    \\end{array}\n",
    "\\right\\} \n",
    "\\end{equation}\n",
    "\n",
    "* The log based loss function $L$ heavily penalizes the wrong result. For instance if the estimated output is $\\hat{y_{i}}=1$ while the target value is $y_{i} = 1$ then the loss penalty is zero, $(0)$ . On the other hand  if the estimated output $\\hat{y_{i}}=0$ while the target value is $y_{i} = 1$ then the penalty is $(\\infty)$.\n",
    " * A more compact version of the loss function is as follows,\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    L(\\hat{y_{i}},y_{i}) = -y_{i}log(\\hat{y_{i}}) -(1-y_{i})log(1-\\hat{y_{i}})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    J(\\bar{w},b)=\\frac{1}{m}\\sum\\limits_{i=1}^m L(\\hat{y_{i}},y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "* Applying partial derivatives to log-loss based cost function, we obtain a result quite similar to linear regression case except for the estimated output expression, $\\hat{y_i}$.\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\frac{\\partial{J(w,b)}}{\\partial{w}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (\\hat{y_i}-y_i)x_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\frac{\\partial{J(w,b)}}{\\partial{b}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (\\hat{y_i}-y_i)\n",
    "\\end{equation}\n",
    "\n",
    "* Remember the fact that the estimated output expression in logistic regression is different than the one in linear regression. Thus we can still use the same code to large extent.\n",
    "\n",
    "| linear regression | logistic regression|\n",
    "| :-----------: |:-----------|\n",
    "| $\\hat{y}=\\bar{w}.\\bar{x}+b$| $\\hat{y}=\\frac{1}{1+e^{-(\\bar{w}.\\bar{x}+b)}}$ \n",
    "\n",
    "* Once you determine the best weights after model fitting, you can get the predicted outputs $\\hat{y_i}$ for any given input feature $x_i$, the predicted output is $(1)$ if $sig(z_i) > 0.5$ and $(0)$ otherwise. The threshold doesn’t have to be $(0.5)$, you might define a lower or higher value in accordance with the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Overfitting Issue\n",
    "* Both linear regression and logistic regression work well for many tasks. But sometimes in an application, the algorithm can run into a problem called overfitting, which can cause it to perform poorly.\n",
    "* <span style=\"color:orange\">Underfitting (high-bias)</span> occurs when the algorithm does not fit to the training data very well. We are biased that the data will fit into a specific  model (most possibly a simpler model). There's a clear pattern in the training data that the algorithm is just unable to capture. Another way to think of this form of bias is as if the learning algorithm has a very strong preconception, or we say a very strong bias.\n",
    "* <span style=\"color:orange\">Overfitting (high-variance)</span> occurs when the model fits the training set very well. The model then learns not only the relationships among data but also the noise in the dataset. However, the model is NOT able to generalize to new examples that's never seen before.\n",
    "* The intuition behind overfitting or high-variance is that the algorithm is trying very hard to fit every single training example. It turns out that if your training set were just even a little bit different,say, in a house prediction problem, one house was priced just a little bit more little bit less, then the function that the algorithm fits could end up being totally different.\n",
    "* For adressing the overfitting issue.\n",
    "    * First you can collect <span style=\"color:orange\">more training data</span>.\n",
    "    * Second option is <span style=\"color:orange\">feature selection</span>, to see if you can use fewer features. One disadvantage of feature selection is that by using only a subset of the features, the algorithm is throwing away some of the information that you have.\n",
    "    * Third option is <span style=\"color:orange\">regularization</span>, where you shrink the values of the model parameters without necessarily demanding that the parameter is set to exactly 0. It turns out that you end up with a curve fitting the training data much better.\n",
    "* Need figures for both regression and classification !!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Regularization\n",
    "* The intuition behind the regularization is to avoid the disadvantage of the feature selection concept. In feature selection, you may get rid of the features causing the high-variance by forcing their weights to zero. However you would be also erasing the information coming from theses features. Alternatively, you may just force weights getting closer to zero instead of setting them exactly zero.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (\\hat{y}^i-y^i)^2 +\\frac{\\lambda}{2m}\\sum\\limits_{i=1}^n (w_j)^2\n",
    "\\end{equation}\n",
    "\n",
    "* The $w_j$ corresponds multiple feature case where the $(j)$ index changes within the range of $(1,n)$\n",
    "* By choosing a large $(\\lambda)$ value, you may force $(w_j)$ values to get closer to zero, which might be a remedy for the overfitting issue.\n",
    "* Although the cost expression for linear and logistic regressions are different, they will take the same form for the use in gradient descent except for the explicit expression of the estimated output, $\\hat{y}$.\n",
    "* Regularized cost function for the linear regression.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    J(w,b)= \\frac{1}{2m}\\sum\\limits_{i=1}^m (\\bar{w}.\\bar{x}_i+b-y_i)^2 +\\frac{\\lambda}{2m}\\sum\\limits_{i=1}^n (w_j)^2\n",
    "\\end{equation}\n",
    "\n",
    "* Regularized cost function for the logistic regression.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    J(\\bar{w},b)=\\frac{1}{m}\\sum\\limits_{i=1}^m [-y_{i}log(\\hat{y_{i}}) -(1-y_{i})log(1-\\hat{y_{i}})] +\\frac{\\lambda}{2m}\\sum\\limits_{i=1}^n (w_j)^2\n",
    "    \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "\\hat{y_i}=\\frac{1}{1+e^{-(\\bar{w}.\\bar{x_i}+b)}}\n",
    "\\end{equation}\n",
    "\n",
    "* The partial gradients for both linear and logistic regression including the effect of the regularization. Although they are implicitly the same equation, remember that the explicite expression of $\\bar{y_i}$ is different.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\frac{\\partial{J(w,b)}}{\\partial{w_j}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (\\hat{y_i}-y_i)x_i +\\frac{\\lambda}{m}w_j\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    \\frac{\\partial{J(w,b)}}{\\partial{b}}= \\frac{1}{m}\\sum\\limits_{i=1}^m (\\hat{y_i}-y_i)\n",
    "\\end{equation}\n",
    "* The next values of weight and biases can be calculates as follows, as the gradient descent algorithm works.\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    w_{k+1,j}=w_{k,j}-\\alpha\\frac{\\partial{J}}{\\partial{w_j}}|_{(w_{k,j},b_{k,j})}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\nonumber\n",
    "    b_{k+1}=b_{k}-\\alpha\\frac{\\partial{J}}{\\partial{b}}|_{(w_{k,j},b_{k,j})}\n",
    "\\end{equation}\n",
    "* **Important Note:** The $(i)$ subscript ranges within $(1,m)$ corresponding to different items of training dataset, the $(j)$ corresponds to multiple feature capability of the model and ranges within $(1,n)$, and the $(k)$ subscript corresponds to iterations as the gradient descent algorithm advances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Unsupervised Learning\n",
    "* While the data comes with both inputs (x) and output labels (y) in supervised learning, the data comes <span style=\"color:orange\"> only with inputs (x)</span> in unsupervised learning  and the learning algorithm has to find some structure, some pattern or <span style=\"color:orange\">something interesting in the data.</span>\n",
    "* After supervised learning, the most widely used form of machine learning is unsupervised learning.\n",
    "* Assume that a dataset is given with patient's tumor size and the patient's age but not whether the tumor is benign or malignant.  Our job is to find some structure or some pattern or just find something interesting in the data. This is unsupervised learning, we call it unsupervised because we're not trying to supervise the algorithm.\n",
    "\n",
    "<p>\n",
    "    <img width=\"48%\" alt=\"peaks_3d\" src=\"assets/images/unsup_vs_sup_learning1.svg\"/> \n",
    "    <img width=\"48%\" alt=\"peaks_contour\" src=\"assets/images/unsup_vs_sup_learning2.svg\"/> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Clustering\n",
    "* The clustering algorithm is a type of unsupervised learning algorithm, which takes data without labels and tries to automatically group them into clusters.\n",
    "* A clustering algorithm groups similar data points together\n",
    "    * Given a database of customer data, automatically discover market segments and group customers into different market segments.\n",
    "    * Given a set of news articles found on the web, group them into sets of articles about the same stories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
